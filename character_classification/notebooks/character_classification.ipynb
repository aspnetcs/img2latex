{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform,io\n",
    "from skimage.filters import threshold_mean\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "IMG_PATH = os.path.join(DATA_PATH, \"extracted_images\")\n",
    "SYMBOL_LIST_PATH = os.path.join(DATA_PATH, \"handwritten_math_symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IMAGE_SIZE = (32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SYMBOL_LIST_PATH) as f:\n",
    "    symbols_list = f.read().split(\"\\n\")\n",
    "num_classes = len(symbols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    # read\n",
    "    im = io.imread(a_img_path)\n",
    "        \n",
    "    # resize\n",
    "    im = transform.resize(im, TARGET_IMAGE_SIZE, mode='symmetric', preserve_range=True)\n",
    "    \n",
    "    # threashold to convert it to binary\n",
    "    thresh = threshold_mean(im)\n",
    "    binary = im > thresh\n",
    "\n",
    "    # binary conversion\n",
    "    binary[binary == True] = 1\n",
    "    binary[binary == False] = 0\n",
    "    \n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharmistha/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "X = [] \n",
    "Y = []\n",
    "\n",
    "for i in range(len(symbols_list)):\n",
    "    for a_img_path in glob.glob(os.path.join(IMG_PATH, symbols_list[i]) + \"/*.jpg\"):\n",
    "        \n",
    "        # read\n",
    "        im = io.imread(a_img_path)\n",
    "        \n",
    "        # resize\n",
    "        im = transform.resize(im, TARGET_IMAGE_SIZE, mode='symmetric', preserve_range=True)\n",
    "        \n",
    "        # threashold to convert it to binary\n",
    "        thresh = threshold_mean(im)\n",
    "        binary = im > thresh\n",
    "        \n",
    "        # binary conversion\n",
    "        binary[binary == True] = 1\n",
    "        binary[binary == False] = 0\n",
    "        \n",
    "        X.append(binary)\n",
    "        Y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375974"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = keras.utils.to_categorical(Y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (263181, 32, 32)\n",
      "X_test (112793, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train \" + str(X_train.shape))\n",
    "print(\"X_test \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (263181, 83)\n",
      "y_test (112793, 83)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train \" + str(y_train.shape))\n",
    "print(\"y_test \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efb200ff2e8>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[90000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,83):\n",
    "    if Y[90000][i] == 1:\n",
    "        wow = i\n",
    "        break\n",
    "symbols_list[wow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(TARGET_IMAGE_SIZE[0],TARGET_IMAGE_SIZE[1],1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_49 (Conv2D)           (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 83)                42579     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 83)                0         \n",
      "=================================================================\n",
      "Total params: 1,287,731\n",
      "Trainable params: 1,287,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = format_train_for_keras_fit(X_train)\n",
    "X_test = format_train_for_keras_fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = ModelCheckpoint('../trained_models/model-epoch:{epoch:02d}-acc:{acc:.3f}-val_acc{val_acc:.3f}.hdf5',\n",
    "                              monitor='val_acc',\n",
    "                              verbose=1,\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True,\n",
    "                              mode='max',\n",
    "                              period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 263181 samples, validate on 112793 samples\n",
      "Epoch 1/10\n",
      "263181/263181 [==============================] - 40s 151us/step - loss: 0.6170 - acc: 0.8346 - val_loss: 0.2129 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.93602, saving model to ../trained_models/model-epoch:01-acc:0.835-val_acc0.936.hdf5\n",
      "Epoch 2/10\n",
      "263181/263181 [==============================] - 40s 150us/step - loss: 0.2668 - acc: 0.9196 - val_loss: 0.1607 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.93602 to 0.95040, saving model to ../trained_models/model-epoch:02-acc:0.920-val_acc0.950.hdf5\n",
      "Epoch 3/10\n",
      "263181/263181 [==============================] - 39s 150us/step - loss: 0.2155 - acc: 0.9331 - val_loss: 0.1322 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.95040 to 0.95869, saving model to ../trained_models/model-epoch:03-acc:0.933-val_acc0.959.hdf5\n",
      "Epoch 4/10\n",
      "263181/263181 [==============================] - 40s 150us/step - loss: 0.1865 - acc: 0.9405 - val_loss: 0.1187 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.95869 to 0.96181, saving model to ../trained_models/model-epoch:04-acc:0.940-val_acc0.962.hdf5\n",
      "Epoch 5/10\n",
      "263181/263181 [==============================] - 40s 150us/step - loss: 0.1673 - acc: 0.9458 - val_loss: 0.1061 - val_acc: 0.9663\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96181 to 0.96627, saving model to ../trained_models/model-epoch:05-acc:0.946-val_acc0.966.hdf5\n",
      "Epoch 6/10\n",
      "263181/263181 [==============================] - 40s 150us/step - loss: 0.1542 - acc: 0.9501 - val_loss: 0.0988 - val_acc: 0.9686\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.96627 to 0.96861, saving model to ../trained_models/model-epoch:06-acc:0.950-val_acc0.969.hdf5\n",
      "Epoch 7/10\n",
      "263181/263181 [==============================] - 40s 150us/step - loss: 0.1419 - acc: 0.9535 - val_loss: 0.0943 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.96861 to 0.97011, saving model to ../trained_models/model-epoch:07-acc:0.953-val_acc0.970.hdf5\n",
      "Epoch 8/10\n",
      "263181/263181 [==============================] - 39s 150us/step - loss: 0.1340 - acc: 0.9558 - val_loss: 0.0848 - val_acc: 0.9726\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.97011 to 0.97262, saving model to ../trained_models/model-epoch:08-acc:0.956-val_acc0.973.hdf5\n",
      "Epoch 9/10\n",
      "263181/263181 [==============================] - 40s 150us/step - loss: 0.1260 - acc: 0.9583 - val_loss: 0.0818 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.97262 to 0.97408, saving model to ../trained_models/model-epoch:09-acc:0.958-val_acc0.974.hdf5\n",
      "Epoch 10/10\n",
      "263181/263181 [==============================] - 40s 150us/step - loss: 0.1211 - acc: 0.9596 - val_loss: 0.0800 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.97408 to 0.97427, saving model to ../trained_models/model-epoch:10-acc:0.960-val_acc0.974.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb14e9be80>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test,y_test),\n",
    "    callbacks= [checkpoints]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model seems to be: model-epoch:06-acc:0.950-val_acc0.969.hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(img):\n",
    "    img[img>0]=1\n",
    "    return img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 751948 images belonging to 83 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        IMG_PATH,\n",
    "        classes=symbols_list,\n",
    "        color_mode='grayscale',\n",
    "        target_size=TARGET_IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch = 751948 // BATCH_SIZE,\n",
    "#         epochs=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_train_for_keras_fit(_X):\n",
    "     return np.transpose(_X.reshape(len(_X), TARGET_IMAGE_SIZE[0], TARGET_IMAGE_SIZE[1], 1), axes=[0,2,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-193-ec29b05cd63f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-193-ec29b05cd63f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Y =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
